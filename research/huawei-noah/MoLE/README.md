
# MoLE
Official code of ''Mixture of Lookup Experts''.

<p align="left">
<a href="https://arxiv.org/abs/2503.15798" alt="arXiv">
    <img src="https://img.shields.io/badge/arXiv-2503.15798-b31b1b.svg?style=flat" /></a>
</p>

MoLE is a novel edge-friendly LLM architecture. With the same number of activated parameters, MoLE achieves:
+ Latency and memory overhead comparable to dense models
+ Performance on par with Mixture-of-Experts (MoE) models.


<p align="center">
<img src="https://arxiv.org/html/2503.15798v1/x2.png" width="700">
</p>
