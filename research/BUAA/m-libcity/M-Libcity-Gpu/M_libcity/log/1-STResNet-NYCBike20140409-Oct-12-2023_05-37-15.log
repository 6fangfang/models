2023-10-12 05:37:15,396 - INFO - Log directory: /home/zhangwt/remote/M-Libcity/M_libcity/log
2023-10-12 05:37:15,396 - INFO - Begin pipeline, task=traffic_state_pred, model_name=STResNet, dataset_name=NYCBike20140409, exp_id=1
2023-10-12 05:37:15,396 - INFO - {'task': 'traffic_state_pred', 'model': 'STResNet', 'dataset': 'NYCBike20140409', 'saved_model': True, 'train': False, 'batch_size': 64, 'exp_id': 1, 'dataset_class': 'STResNetDataset', 'executor': 'TrafficStateExecutor', 'evaluator': 'TrafficStateEvaluator', 'nb_residual_unit': 12, 'batch_norm': False, 'scaler': 'minmax11', 'load_external': True, 'normal_external': False, 'ext_scaler': 'none', 'add_time_in_day': False, 'add_day_in_week': False, 'external_time': True, 'max_epoch': 500, 'learner': 'adam', 'learning_rate': 0.0002, 'lr_decay': False, 'clip_grad_norm': True, 'max_grad_norm': 0.1, 'use_early_stop': True, 'patience': 50, 'cache_dataset': True, 'num_workers': 1, 'pad_with_last_sample': True, 'train_rate': 0.8, 'eval_rate': 0.1, 'input_window': 12, 'output_window': 12, 'len_closeness': 4, 'len_period': 2, 'len_trend': 0, 'interval_period': 1, 'interval_trend': 7, 'use_row_column': True, 'gpu': True, 'gpu_id': 0, 'train_loss': 'none', 'epoch': 0, 'weight_decay': 0, 'lr_epsilon': 1e-08, 'lr_beta1': 0.9, 'lr_beta2': 0.999, 'lr_alpha': 0.99, 'lr_momentum': 0.0, 'lr_scheduler': 'multisteplr', 'lr_decay_ratio': 0.1, 'steps': [5, 20, 40, 70], 'step_size': 10, 'lr_T_max': 30, 'lr_eta_min': 0, 'lr_patience': 10, 'lr_threshold': 0.0001, 'log_level': 'INFO', 'log_every': 1, 'load_best_epoch': True, 'hyper_tune': False, 'metrics': ['MAE', 'MAPE', 'RMSE'], 'evaluator_mode': 'single', 'save_mode': ['csv'], 'geo': {'including_types': ['Polygon'], 'Polygon': {'row_id': 'num', 'column_id': 'num'}}, 'grid': {'including_types': ['state'], 'state': {'row_id': 16, 'column_id': 8, 'new_flow': 'num', 'end_flow': 'num'}}, 'data_col': ['new_flow', 'end_flow'], 'data_files': ['NYCBIKE20140409'], 'geo_file': 'NYCBIKE20140409', 'output_dim': 2, 'time_intervals': 3600, 'init_weight_inf_or_zero': 'inf', 'set_weight_link_or_dist': 'dist', 'calculate_weight_adj': False, 'weight_adj_epsilon': 0.1}
2023-10-12 05:37:15,467 - INFO - Loaded file NYCBIKE20140409.geo, num_grids=128, grid_size=(16, 8)
2023-10-12 05:37:15,468 - INFO - Generate grid rel file, shape=(128, 128)
2023-10-12 05:37:15,469 - INFO - Loading /home/zhangwt/remote/M-Libcity/M_libcity/cache/dataset_cache/grid_based_NYCBike20140409_12_12_0.8_0.1_minmax11_64_True_False_False_True_True_4_2_0_1_7.npz
2023-10-12 05:37:15,769 - INFO - train	x: (3475, 6, 16, 8, 2), y: (3475, 1, 16, 8, 2), x_ext: (3475, 6, 33), y_ext: (3475, 33)
2023-10-12 05:37:15,770 - INFO - eval	x: (435, 6, 16, 8, 2), y: (435, 1, 16, 8, 2), x_ext: (435, 6, 33), y_ext: (435, 33)
2023-10-12 05:37:15,770 - INFO - test	x: (434, 6, 16, 8, 2), y: (434, 1, 16, 8, 2), x_ext: (434, 6, 33), y_ext: (434, 33)
2023-10-12 05:37:15,779 - INFO - MinMax11Scaler max: 267.0, min: 0.0
2023-10-12 05:37:15,779 - INFO - NoneScaler
2023-10-12 05:37:18,787 - INFO - STResNet<
  (network): STResNet_model<
    (relu): ReLU<>
    (tanh): Tanh<>
    (c_way): SequentialCell<
      (conv1): Conv2d<input_channels=8, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
      (ResUnits): ResUnits<
        (residual_units): SequentialCell<
          (0): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (1): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (2): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (3): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (4): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (5): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (6): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (7): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (8): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (9): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (10): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (11): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.c_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          >
        >
      (relu): ReLU<>
      (conv2): Conv2d<input_channels=64, output_channels=2, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
      (FusionLayer): TrainableEltwiseLayer<>
      >
    (p_way): SequentialCell<
      (conv1): Conv2d<input_channels=4, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
      (ResUnits): ResUnits<
        (residual_units): SequentialCell<
          (0): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (1): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (2): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (3): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (4): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (5): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (6): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (7): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (8): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (9): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (10): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          (11): ResidualUnit<
            (bn_relu_conv1): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            (bn_relu_conv2): BnReluConv<
              (bn1): BatchNorm2d<num_features=64, eps=1e-05, momentum=0.9, gamma=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.gamma, shape=(64,), dtype=Float32, requires_grad=True), beta=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.beta, shape=(64,), dtype=Float32, requires_grad=True), moving_mean=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.moving_mean, shape=(64,), dtype=Float32, requires_grad=False), moving_variance=Parameter (name=network.p_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.moving_variance, shape=(64,), dtype=Float32, requires_grad=False)>
              (relu): ReLU<>
              (conv1): Conv2d<input_channels=64, output_channels=64, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
              >
            >
          >
        >
      (relu): ReLU<>
      (conv2): Conv2d<input_channels=64, output_channels=2, kernel_size=(3, 3), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=normal, bias_init=zeros, format=NCHW>
      (FusionLayer): TrainableEltwiseLayer<>
      >
    (external_ops): SequentialCell<
      (embd): Dense<input_channels=33, output_channels=10, has_bias=True>
      (relu1): ReLU<>
      (fc): Dense<input_channels=10, output_channels=256, has_bias=True>
      (relu2): ReLU<>
      >
    >
  >
2023-10-12 05:37:18,795 - INFO - network.c_way.conv1.weight	(64, 8, 3, 3)	True
2023-10-12 05:37:18,795 - INFO - network.c_way.conv1.bias	(64,)	True
2023-10-12 05:37:18,795 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,795 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,795 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,795 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.0.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.1.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.2.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,796 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.3.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.4.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.5.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,797 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.6.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.7.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.8.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,798 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.9.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.10.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.ResUnits.residual_units.11.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.conv2.weight	(2, 64, 3, 3)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.conv2.bias	(2,)	True
2023-10-12 05:37:18,799 - INFO - network.c_way.FusionLayer.weights	(1, 2, 16, 8)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.conv1.weight	(64, 4, 3, 3)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.conv1.bias	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.0.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.1.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,800 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.2.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.3.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.4.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,801 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.5.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.6.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.7.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,802 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.8.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.9.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.10.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.gamma	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv1.bn1.beta	(64,)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv1.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,803 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv1.conv1.bias	(64,)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.gamma	(64,)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv2.bn1.beta	(64,)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv2.conv1.weight	(64, 64, 3, 3)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.ResUnits.residual_units.11.bn_relu_conv2.conv1.bias	(64,)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.conv2.weight	(2, 64, 3, 3)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.conv2.bias	(2,)	True
2023-10-12 05:37:18,804 - INFO - network.p_way.FusionLayer.weights	(1, 2, 16, 8)	True
2023-10-12 05:37:18,804 - INFO - network.external_ops.embd.weight	(10, 33)	True
2023-10-12 05:37:18,804 - INFO - network.external_ops.embd.bias	(10,)	True
2023-10-12 05:37:18,804 - INFO - network.external_ops.fc.weight	(256, 10)	True
2023-10-12 05:37:18,804 - INFO - network.external_ops.fc.bias	(256,)	True
2023-10-12 05:37:18,805 - INFO - Total parameter numbers: 1791704
2023-10-12 05:37:18,805 - INFO - You select `adam` optimizer.
2023-10-12 05:37:19,371 - WARNING - Received none train loss func and will use the loss func defined in the model.
2023-10-12 05:37:19,372 - INFO - Loaded model at /home/zhangwt/remote/M-Libcity/M_libcity/cache/1/model_cache/STResNet_NYCBike20140409.ckpt
2023-10-12 05:37:19,826 - INFO - Start evaluating ...
2023-10-12 05:37:21,563 - INFO - Note that you select the single mode to evaluate!
2023-10-12 05:37:21,576 - INFO - Evaluate result is saved at /home/zhangwt/remote/M-Libcity/M_libcity/cache/1/evaluate_cache/2023_10_12_05_37_21_STResNet_NYCBike20140409.csv
2023-10-12 05:37:21,581 - INFO - 
         MAE        MAPE      RMSE
1  2.4519145  0.24904248  5.250014
